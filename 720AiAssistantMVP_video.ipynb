{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44210841",
   "metadata": {},
   "source": [
    "# 720 AI Assistant MVP — Video Pose Analysis (MediaPipe)\n",
    "\n",
    "This notebook processes a **video** (not just photos):\n",
    "- Runs **MediaPipe PoseLandmarker** per frame\n",
    "- Saves an **annotated video** with landmarks\n",
    "- Computes simple technique metrics and prints **insights/cues**\n",
    "\n",
    "> Update `VIDEO_PATH` to your local file name (upload it to the same folder as the notebook or mount it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd14d01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9012dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acfdf0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d8a07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: pose_landmarker_heavy.task\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, os, pathlib\n",
    "\n",
    "MODEL_URL = \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\"\n",
    "MODEL_PATH = \"pose_landmarker_heavy.task\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
    "    print(\"Downloaded:\", MODEL_PATH)\n",
    "else:\n",
    "    print(\"Model already exists:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f60de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing helper (MediaPipe official style)\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    pose_landmarks_list = detection_result.pose_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    for idx in range(len(pose_landmarks_list)):\n",
    "        pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        pose_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z, visibility=getattr(lm, \"visibility\", 0.0))\n",
    "            for lm in pose_landmarks\n",
    "        ])\n",
    "\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            pose_landmarks_proto,\n",
    "            solutions.pose.POSE_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "    return annotated_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dab2f7",
   "metadata": {},
   "source": [
    "## Run on a video\n",
    "\n",
    "1) Set `VIDEO_PATH`\n",
    "2) Run the cell — it will create:\n",
    "- `annotated_output.mp4`\n",
    "- `metrics.csv`\n",
    "- Printed insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d4c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Frames: 106 | Frames with pose: 106\n",
      "Saved: annotated_output.mp4\n",
      "Saved: metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>head_drop</th>\n",
       "      <th>elbow_tuck</th>\n",
       "      <th>arm_open</th>\n",
       "      <th>torso_lean</th>\n",
       "      <th>kick_lateral</th>\n",
       "      <th>rot_proxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.242433</td>\n",
       "      <td>0.760600</td>\n",
       "      <td>0.464836</td>\n",
       "      <td>0.074764</td>\n",
       "      <td>0.838851</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033333</td>\n",
       "      <td>-0.246170</td>\n",
       "      <td>0.748190</td>\n",
       "      <td>0.453997</td>\n",
       "      <td>0.075691</td>\n",
       "      <td>0.827802</td>\n",
       "      <td>0.002731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>-0.241542</td>\n",
       "      <td>0.705140</td>\n",
       "      <td>0.407603</td>\n",
       "      <td>0.078757</td>\n",
       "      <td>0.711475</td>\n",
       "      <td>0.007264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.217436</td>\n",
       "      <td>0.680718</td>\n",
       "      <td>0.385812</td>\n",
       "      <td>0.087183</td>\n",
       "      <td>0.648729</td>\n",
       "      <td>0.015019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>-0.226747</td>\n",
       "      <td>0.662409</td>\n",
       "      <td>0.368128</td>\n",
       "      <td>0.090305</td>\n",
       "      <td>0.621979</td>\n",
       "      <td>0.002655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          t  head_drop  elbow_tuck  arm_open  torso_lean  kick_lateral  \\\n",
       "0  0.000000  -0.242433    0.760600  0.464836    0.074764      0.838851   \n",
       "1  0.033333  -0.246170    0.748190  0.453997    0.075691      0.827802   \n",
       "2  0.066667  -0.241542    0.705140  0.407603    0.078757      0.711475   \n",
       "3  0.100000  -0.217436    0.680718  0.385812    0.087183      0.648729   \n",
       "4  0.133333  -0.226747    0.662409  0.368128    0.090305      0.621979   \n",
       "\n",
       "   rot_proxy  \n",
       "0   0.000000  \n",
       "1   0.002731  \n",
       "2   0.007264  \n",
       "3   0.015019  \n",
       "4   0.002655  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List\n",
    "\n",
    "# ========= 1) INPUT =========\n",
    "VIDEO_PATH = \"swing720_attempt.mp4\"   # <-- change this to your file\n",
    "OUTPUT_VIDEO = \"annotated_output.mp4\"\n",
    "OUTPUT_METRICS = \"metrics.csv\"\n",
    "\n",
    "# ========= 2) PoseLandmarker (VIDEO mode) =========\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=MODEL_PATH),\n",
    "    running_mode=VisionRunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "landmarker = PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# ========= 3) Landmark indices =========\n",
    "NOSE = 0\n",
    "L_SHOULDER, R_SHOULDER = 11, 12\n",
    "L_ELBOW, R_ELBOW = 13, 14\n",
    "L_WRIST, R_WRIST = 15, 16\n",
    "L_HIP, R_HIP = 23, 24\n",
    "L_ANKLE, R_ANKLE = 27, 28\n",
    "\n",
    "def lm_xy(lms, idx):\n",
    "    lm = lms[idx]\n",
    "    return np.array([lm.x, lm.y], dtype=np.float32)\n",
    "\n",
    "def dist(a, b):\n",
    "    return float(np.linalg.norm(a - b))\n",
    "\n",
    "@dataclass\n",
    "class FrameMetrics:\n",
    "    t: float\n",
    "    head_drop: float      # proxy for \"looking down\" / chest collapsing\n",
    "    elbow_tuck: float     # proxy for elbows in (lower is better)\n",
    "    arm_open: float       # proxy for arms open (higher is more open)\n",
    "    torso_lean: float     # radians from vertical (higher = more inclined)\n",
    "    kick_lateral: float   # proxy for kick going to the side\n",
    "    rot_proxy: float      # proxy for rotation per-frame (higher = faster)\n",
    "\n",
    "def compute_frame_metrics(lms, t: float, prev_state: Optional[Dict]=None):\n",
    "    nose = lm_xy(lms, NOSE)\n",
    "    ls, rs = lm_xy(lms, L_SHOULDER), lm_xy(lms, R_SHOULDER)\n",
    "    le, re = lm_xy(lms, L_ELBOW), lm_xy(lms, R_ELBOW)\n",
    "    lw, rw = lm_xy(lms, L_WRIST), lm_xy(lms, R_WRIST)\n",
    "    lh, rh = lm_xy(lms, L_HIP), lm_xy(lms, R_HIP)\n",
    "    la, ra = lm_xy(lms, L_ANKLE), lm_xy(lms, R_ANKLE)\n",
    "\n",
    "    shoulder_mid = (ls + rs) / 2.0\n",
    "    hip_mid = (lh + rh) / 2.0\n",
    "\n",
    "    sh_w = dist(ls, rs)\n",
    "    sh_w = sh_w if sh_w > 1e-6 else 1e-6\n",
    "\n",
    "    # 1) Head drop proxy: nose lower than shoulder mid (y grows downward)\n",
    "    head_drop = float((nose[1] - shoulder_mid[1]) / sh_w)\n",
    "\n",
    "    # 2) Elbow tuck proxy\n",
    "    elbow_tuck = float(((dist(le, shoulder_mid) + dist(re, shoulder_mid)) / 2.0) / sh_w)\n",
    "\n",
    "    # 3) Arm open proxy: wrist far from shoulder\n",
    "    arm_open = float(((dist(lw, ls) + dist(rw, rs)) / 2.0) / sh_w)\n",
    "\n",
    "    # 4) Torso lean angle to vertical\n",
    "    v = shoulder_mid - hip_mid\n",
    "    v_norm = np.linalg.norm(v) + 1e-6\n",
    "    cos_to_vertical = abs(np.dot(v / v_norm, np.array([0.0, -1.0], dtype=np.float32)))\n",
    "    torso_lean = float(np.arccos(np.clip(cos_to_vertical, 0, 1)))  # radians\n",
    "\n",
    "    # 5) Kick lateral proxy: ankles horizontal deviation from hips\n",
    "    kick_lateral = float((abs(la[0] - hip_mid[0]) + abs(ra[0] - hip_mid[0])) / sh_w)\n",
    "\n",
    "    # 6) Rotation proxy: change in shoulder line angle\n",
    "    shoulder_vec = rs - ls\n",
    "    shoulder_angle = float(np.arctan2(shoulder_vec[1], shoulder_vec[0]))\n",
    "    rot_proxy = 0.0\n",
    "    if prev_state and \"shoulder_angle\" in prev_state:\n",
    "        d = shoulder_angle - prev_state[\"shoulder_angle\"]\n",
    "        d = (d + np.pi) % (2*np.pi) - np.pi\n",
    "        rot_proxy = abs(float(d))\n",
    "\n",
    "    state = {\"shoulder_angle\": shoulder_angle}\n",
    "\n",
    "    return FrameMetrics(t, head_drop, elbow_tuck, arm_open, torso_lean, kick_lateral, rot_proxy), state\n",
    "\n",
    "# ========= 4) Read video and write annotated output =========\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Could not open video: {VIDEO_PATH}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (w, h))\n",
    "\n",
    "rows: List[dict] = []\n",
    "prev_state = None\n",
    "frame_idx = 0\n",
    "pose_frames = 0\n",
    "\n",
    "while True:\n",
    "    ok, frame_bgr = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "\n",
    "    t_ms = int((frame_idx / fps) * 1000)\n",
    "    result = landmarker.detect_for_video(mp_image, t_ms)\n",
    "\n",
    "    if result.pose_landmarks and len(result.pose_landmarks) > 0:\n",
    "        pose_frames += 1\n",
    "        lms = result.pose_landmarks[0]\n",
    "        t = frame_idx / fps\n",
    "        m, prev_state = compute_frame_metrics(lms, t, prev_state)\n",
    "        rows.append(m.__dict__)\n",
    "\n",
    "        annotated_rgb = draw_landmarks_on_image(frame_rgb, result)\n",
    "        annotated_bgr = cv2.cvtColor(annotated_rgb, cv2.COLOR_RGB2BGR)\n",
    "        writer.write(annotated_bgr)\n",
    "    else:\n",
    "        # no pose -> write original frame\n",
    "        writer.write(frame_bgr)\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUTPUT_METRICS, index=False)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Frames:\", frame_idx, \"| Frames with pose:\", pose_frames)\n",
    "print(\"Saved:\", OUTPUT_VIDEO)\n",
    "print(\"Saved:\", OUTPUT_METRICS)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039256f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ Elbows not tucked (proxy). Cue: pin elbows to ribs during the spin.\n",
      "❗ Kick goes too lateral (proxy). Cue: drive kick more backward/controlled, keep hips square.\n",
      "\n",
      "Summary: {'avg_head_drop': -0.34332185983657837, 'avg_elbow_tuck': 0.9731638431549072, 'avg_arm_open': 1.1120712757110596, 'avg_torso_lean_rad': 0.19476869702339172, 'avg_kick_lateral': 1.5466331243515015, 'avg_rot_proxy': 0.1301153600215912}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def pct_over(arr, thr): \n",
    "    return float(np.mean(arr > thr)) if len(arr) else 0.0\n",
    "\n",
    "def insights_from_metrics(df: pd.DataFrame):\n",
    "    if df is None or df.empty:\n",
    "        return [\"No pose frames detected. Ensure full body is visible and the video is not too dark/blurred.\"]\n",
    "\n",
    "    head = df[\"head_drop\"].to_numpy(np.float32)\n",
    "    tuck = df[\"elbow_tuck\"].to_numpy(np.float32)\n",
    "    open_ = df[\"arm_open\"].to_numpy(np.float32)\n",
    "    lean = df[\"torso_lean\"].to_numpy(np.float32)\n",
    "    kick = df[\"kick_lateral\"].to_numpy(np.float32)\n",
    "    rot = df[\"rot_proxy\"].to_numpy(np.float32)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    # Starter thresholds (tune with your own videos)\n",
    "    if pct_over(head, 0.10) > 0.35:\n",
    "        out.append(\"❗ Head drops a lot (proxy): you likely look down / collapse chest. Cue: keep chin neutral, look forward.\")\n",
    "    if pct_over(tuck, 0.55) > 0.40:\n",
    "        out.append(\"❗ Elbows not tucked (proxy). Cue: pin elbows to ribs during the spin.\")\n",
    "    if pct_over(open_, 0.85) > 0.45:\n",
    "        out.append(\"❗ Arms open too early (proxy). Cue: stay closed longer before you open for landing.\")\n",
    "    if pct_over(lean, 0.60) > 0.30:\n",
    "        out.append(\"❗ Torso too inclined (proxy). Cue: brace core, avoid throwing the chest down on takeoff.\")\n",
    "    if pct_over(kick, 1.00) > 0.35:\n",
    "        out.append(\"❗ Kick goes too lateral (proxy). Cue: drive kick more backward/controlled, keep hips square.\")\n",
    "    if float(np.mean(rot)) < 0.02:\n",
    "        out.append(\"❗ Rotation looks slow (proxy). Cue: earlier shoulder+hip snap at takeoff; close arms fast.\")\n",
    "\n",
    "    if not out:\n",
    "        out.append(\"✅ No major flags from these simple proxies. Next step: detect phases (takeoff/airborne/landing) for more precise cues.\")\n",
    "\n",
    "    # Summary stats (helps tuning)\n",
    "    summary = {\n",
    "        \"avg_head_drop\": float(np.mean(head)),\n",
    "        \"avg_elbow_tuck\": float(np.mean(tuck)),\n",
    "        \"avg_arm_open\": float(np.mean(open_)),\n",
    "        \"avg_torso_lean_rad\": float(np.mean(lean)),\n",
    "        \"avg_kick_lateral\": float(np.mean(kick)),\n",
    "        \"avg_rot_proxy\": float(np.mean(rot)),\n",
    "    }\n",
    "\n",
    "    return out, summary\n",
    "\n",
    "ins, summary = insights_from_metrics(df)\n",
    "print(\"\\n\".join(ins))\n",
    "print(\"\\nSummary:\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
